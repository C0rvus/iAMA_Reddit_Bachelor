This file contains information about how to use the scripts..

------------------
------------------
Filename:		analyze_thread_lifeSpan_n_average_commentTime_pieChart.py {year} {calc} {time}

Description: 		This script allows you to calculate two things.
			1. The amount of time how long a thread lasted (Difference between thread creation time stamp and the time stamp of the last comment)
			2. The average mean of comment time.. Means, how long it takes until a comment (regardless whether it is a question or not) has been posted within the thread

Arguments:
	
			{year} 		= 		the year which is to be used for the calculation
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{calc}		= 		the data you want to calculate
					=		lifespan || comment
					=		lifespan if you want to calculate the lifespan of a thread, comment is when you want to calculate the average mean comment time within the thread


			{time}		=		the time units in which the calculated values will be seperated into.. (necessary for graph plotting)
					=		min || hours || days
					=		min is for minutes, hours is for separation into hours, days is for seperation into days


Usage example: 		python analyze_thread_lifeSpan_n_average_commentTime_pieChart.py 2009 lifespan days
------------------
------------------
Filename:		top100_pieChart.py {year} {top / worst}

Description: 		Plots a pie chart containing the top 100 / worst 100 questions of the year and the proportion to the answers they have received from the iAMA host.
Arguments:
	
			{year} 		= 	the year which is to be used for the calculation
					=	2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{top / worst)	= 	top [top 100] || worst [worst 100]

Usage example: 		python top100_pieChart.py 2009 false
------------------
------------------
Filename:		analyze_tier_answered_time_pieChart.py {year} {tier} {time}

Description:		Plots a pie chart containing the arithmetic mean of the iama hosts response time to given questions for tier 1 / tier x / tier any in minutes / hours.
Arguments:

			{year}		=		the year which is to be used for the calculation
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{tier}		=		the tier which will be looked at for calculation
					=		1 || x || any
					=		1 only looks at the first tier, x looks on any other tier except tier 1, any looks on all tiers

			{time}		=		the time units in which the calculated values will be seperated into.. (necessary for graph plotting)
					=		min || hours
					=		min is for minutes, hours is for separation into hours

Usage example: 		python analyze_tier_answered_time_pieChart.py 2009 x min
------------------
------------------
Filename:		analyze_thread_lifeSpan_n_average_commentTime_pieChart.py.py {year} {tier} {time} {plot_x_limit}

Description:		Plots a scatter chart which contains values of the upvotes of answered questions (by the iAMA host) and the repsonse time of the iAma host to those questions
			Additionally Pearsons Ro will be calculated and printed into the plots title bar.
Arguments:

			{year}		=		the year which is to be used for the calculation
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{tier}		=		the tier which will be looked at for calculation
					=		1 || x || any
					=		1 only looks at the first tier, x looks on any other tier except tier 1, any looks on all tiers

			{time}		=		the time units in which the calculated values will be seperated into.. (necessary for graph plotting)
					=		min || hours
					=		min is for minutes, hours is for separation into hours

			{plot_x_limit}	=		limits the plot on the x scale.. Useful if you only want to look at given response times..
					=		0 || (int)
					=		0 means no limit; Any other integer limits the plot to the given number on x scale

Usage example: 		python analyze_thread_lifeSpan_n_average_commentTime_pieChart.py.py 2009 any min 300
------------------
------------------
Filename:		analyze_tier_answered_percentage_pieChart.py {year} {tier}

Description:		Plots a pieChart which contains the distribution of questions answered per tier for the given year.

Arguments:

			{year}		=		the year which is to be used for the calculation
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{tier}		=		the tier which will be looked at for calculation
					=		1 || x || any
					=		1 only looks at the first tier, x looks on any other tier except tier 1, any looks on all tiers

Usage example: 		python analyze_tier_answered_percentage_pieChart.py 2009 any
------------------
------------------
Filename:		tier_question_distribution_pieChart.py {year}

Description:		Plots a pieChart which shows the distrubtion of questions on tier 1 and the rest

Arguments:

			{year}		=		the year which is to be used for the calculation
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

Usage example: 		python analyze_tier_question_distribution_pieChart.py 2009
------------------
------------------

This file contains information about how to use the scripts..

------------------
------------------
Filename:		crawl_threads_n_comments.py {crawl_type} {year_begin} {year_end} {shift_hours}

Description: 		This script crawls threads / comments from the reddit API and writes them into the mongoDB.
			This works the way, that you define a beginning year to crawl from an ending year where the crawl ends and the amount of hours the crawler should move on after every successfull query against the reddit servers.
			Whenever the crawler receives an positive response and gets threads in return this script will iterate over them, checks, if they already exist within the mongoDB (and if they are up to date) and (re)creates them if necessary.
			This script will automatically generate the database(s) which will store the crawled information.
			I.e. if the crawler processes information with the timestamp of the year 2009 it will write the information into the database with the name "iAMA_Reddit_Threads_{the appropriate year}".
			The {appropriate year} is calculated from the timestamp of a post.
			I.e. you want to crawl thread information from 2009 - 2012: In the year of 2009 the script will write data into "iAMA_Reddit_Threads_2009" database.. If it processes threads from year 2010 it will write the data into "iAMA_Reddit_Threads_2010" and so on.. Note: If you crawl from 2009 - any year : That "any year" will also be crawled..
			A document within the database has the name of the actually processed thread. Looking inside the document you will only see one collection which holds the following information:
			Be aware ! Crawling takes very long (many days !!) and eats a lot of RAM resources... I suggest you to pick up my already crawled dataset :)
			
			DB values for threads [iAMA_Reddit_Threads_{year}]: 
			
			"_id"		=		The dynamically generated id from the mongo db
			"author"	=		The author of the thread
			"created_utc"	=		The timestamp when that thread has been created (timestamp is in unix epoch formatation)
			"downs"		=		The amount of downvotes that thread has received
			"num_Comments"	=		The amount of comments that thread has received.. (This number differs from the actually parsed comments [this is because reddit does not allow you to crawl already deleted comments...]
			"selftext"	=		The selftext of the thread. In the early years of reddit there were no selftext, therefore that value may be empty in some databases
			"title"		=		The iAMAs title
			"ups"		=		The amount of upvotes

			DB values for comments [iAMA_Reddit_Comments_{year}]:
			
			"_id"		=		The dynamically generated id from the mongo db
			"author"	=		The author of the thread
			"body"		=		The text of the comment
			"name"		=		The id of the thread within the hierarchy [i.e. t1_c09vabt]
			"parent_id"	=		The id to which that comment relates to in the hierarchy [i.e. t3_8nron]
			"ups"		=		The amount of upvotes




Arguments:

			{crawl_type}	= 		the type of data you want to be crawled and written into the database
					=		threads || comments
	
			{year_begin}	= 		The year you want the start the crawling process on
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{year_end}	= 		The year you want the crawling process to stop. The year defined here is included (!!) within the crawling process..
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{shift_hours}	=		The time units (hours) the crawler will move forward in crawling.. Because crawling stepwise asks the reddit server for new data, it is necessary to do this is little intervals.. a value of 96 is good
					=		{int}


Usage example: 		python crawl_threads_n_comments.py threads 2009 2014 96
------------------
------------------

------------------
------------------
Filename:		crawl_differences.py {year_begin} {year_end} {direction}

Description: 		This script crawls missing threads / collections from the reddit servers. Because by regular crawling some threads will be missed due to some limitions of the amazon cloudsearch (which will be used for crawling that data) the [iAMA_Reddit_Threads_{year}] and [iAMA_Reddit_Comments_{year}] do not contain the same amount of collections. To "fix" this issue this script compares the collections of both databases [according to the given year] and crawls the missing data.


Arguments:

			{year_begin}	= 		The year you want the start the crawling process on
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{year_end}	= 		The year you want the crawling process to stop. The year defined here is included (!!) within the crawling process..
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{direction}	=		Defines the direction in which the comparison and crawling process should be started (from the last to the first collection - and vice versa). This is helpful if you want to speed up the crawling process so you can start one crawler forward and the other one backward. The scripts have a fallback mechanism which enables them to not write information twice into the database. So before every write process into the database it will be checked whether that actually processed collection already exists in the database or not.

					=		forward || backward


Usage example: 		python crawl_differences.py 2009 2010 backward
------------------
------------------
