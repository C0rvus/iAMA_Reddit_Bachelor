This file contains information about how to use the scripts..

------------------
------------------
Filename:		thread_n_comment_crawler.py {crawl_type} {year_begin} {year_end} {shift_hours}

Description: 		This script crawls threads / comments from the reddit API and writes them into the mongoDB.
			This works the way, that you define a beginning year to crawl from an ending year where the crawl ends and the amount of hours the crawler should move on after every successfull query against the reddit servers.
			Whenever the crawler receives an positive response and gets threads in return this script will iterate over them, checks, if they already exist within the mongoDB (and if they are up to date) and (re)creates them if necessary.
			This script will automatically generate the database(s) which will store the crawled information.
			I.e. if the crawler processes information with the timestamp of the year 2009 it will write the information into the database with the name "iAMA_Reddit_Threads_{the appropriate year}".
			The {appropriate year} is calculated from the timestamp of a post.
			I.e. you want to crawl thread information from 2009 - 2012: In the year of 2009 the script will write data into "iAMA_Reddit_Threads_2009" database.. If it processes threads from year 2010 it will write the data into "iAMA_Reddit_Threads_2010" and so on.. Note: If you crawl from 2009 - any year : That "any year" will also be crawled..
			A document within the database has the name of the actually processed thread. Looking inside the document you will only see one collection which holds the following information:
			Be aware ! Crawling takes very long (many days !!) and eats a lot of RAM resources... I suggest you to pick up my already crawled dataset :)
			
			DB values for threads [iAMA_Reddit_Threads_{year}]: 
			
			"_id"		=		The dynamically generated id from the mongo db
			"author"	=		The author of the thread
			"created_utc"	=		The timestamp when that thread has been created (timestamp is in unix epoch formatation)
			"downs"		=		The amount of downvotes that thread has received
			"num_Comments"	=		The amount of comments that thread has received.. (This number differs from the actually parsed comments [this is because reddit does not allow you to crawl already deleted comments...]
			"selftext"	=		The selftext of the thread. In the early years of reddit there were no selftext, therefore that value may be empty in some databases
			"title"		=		The iAMAs title
			"ups"		=		The amount of upvotes

			DB values for comments [iAMA_Reddit_Comments_{year}]:
			
			"_id"		=		The dynamically generated id from the mongo db
			"author"	=		The author of the thread
			"body"		=		The text of the comment
			"name"		=		The id of the thread within the hierarchy [i.e. t1_c09vabt]
			"parent_id"	=		The id to which that comment relates to in the hierarchy [i.e. t3_8nron]
			"ups"		=		The amount of upvotes




Arguments:

			{crawl_type}	= 		the type of data you want to be crawled and written into the database
					=		threads || comments
	
			{year_begin}	= 		The year you want the start the crawling process on
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{year_end}	= 		The year you want the crawling process to stop. The year defined here is included (!!) within the crawling process..
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{shift_hours}	=		The time units (hours) the crawler will move forward in crawling.. Because crawling stepwise asks the reddit server for new data, it is necessary to do this is little intervals.. a value of 96 is good
					=		{int}


Usage example: 		python thread_n_comment_crawler.py threads 2009 2014 96
------------------
------------------

------------------
------------------
Filename:		crawl_that_diff.py {year_begin} {year_end} {direction}

Description: 		This script crawls missing threads / collections from the reddit servers. Because by regular crawling some threads will be missed due to some limitions of the amazon cloudsearch (which will be used for crawling that data) the [iAMA_Reddit_Threads_{year}] and [iAMA_Reddit_Comments_{year}] do not contain the same amount of collections. To "fix" this issue this script compares the collections of both databases [according to the given year] and crawls the missing data.


Arguments:

			{year_begin}	= 		The year you want the start the crawling process on
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{year_end}	= 		The year you want the crawling process to stop. The year defined here is included (!!) within the crawling process..
					=		2009 || 2010 || 2011 || 2012 || 2013 || 2014 || 2015 || 2016

			{direction}	=		Defines the direction in which the comparison and crawling process should be started (from the last to the first collection - and vice versa). This is helpful if you want to speed up the crawling process so you can start one crawler forward and the other one backward. The scripts have a fallback mechanism which enables them to not write information twice into the database. So before every write process into the database it will be checked whether that actually processed collection already exists in the database or not.

					=		forward || backward


Usage example: 		python crawl_that_diff.py 2009 2010 backward
------------------
------------------